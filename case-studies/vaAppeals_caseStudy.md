
# MODERINIZING VA APPEALS DIGITAL SYSTEM
## BUILD A VENMO CLONE IN 72 HOURS.  MEET ALL REQUIREMENTS. WIN

###Summary
The Veteran Affair’s Enterprise Appeals Process was inefficient and did not adequately support the adjudication of appeals.  Customers using the system encountered several issues that prevented them from completing the appeals process online.

The U.S. Digital Service embedded a team at the VA to help modernize the VA’s Enterprise Appeals Process and enable VA to adjudicate appeals efficiently.  The team began to quickly develop remedies to fix the system. In order to continue the agile pace of work, they needed to quickly add developers to execute the project on the scale needed.

Therefore, they ran a coding challenge to select the right team to support the existing team and scale the work.
Problem

The Digital Service team was unable to maintain the scale and agile pace of work with two developers.  The team needed to rapidly on board developers to continue modernizing the VA’s Enterprise Appeals Process.  In order to do so, they needed to quickly identify the best qualified vendor who could provide developers with the skillset needed.

### Action/Solution

The team performed market research via an RFI to determine the most appropriate contract strategy and concluded that a Service-Disabled Veteran-Owned Small Business set aside on VA’s Transformation Twenty-One Total Technology Next Generation Contract would be most advantageous.  This contract proved to provide an appropriate number of vendors with the requisite capabilities.
A challenge was a viable option because there was technical expertise available to evaluate the code submissions. They executed the challenge under the following conditions:

1.	Request for Task Execution Plan (RTEP) detailing the full requirement was released to all vendors.  This included the coding exercise submission instructions and evaluation approach. At this point the vendors could begin working on the response to the price portion of the evaluation and the written technical portion.  This provided guidance on the coding submission and detailed how the submission would be evaluated, but did not provide the information to begin the coding exercise.
2.	After release of the RTEP, VA provided notice that the coding submission information would be posted at a certain date/time.
3.	At the appointed time, nine user stories were released and vendors were provided 72 hours to provide a coding submission.  Upon release, vendors were provided 4 hours to submit questions pertaining to the coding submission.  VA had 2 hours to respond to all questions.
4.	Challenge required the offerors to build a venmo clone in 72 hours.  The clone required a lot security considerations, so there were a lot of opportunities for mistakes OR excellence!
5.	A detailed rubric was developed to guide offerors on the requirements for successful completion of the challenge.
6.	The source code for the coding submission and all relevant design assets and documentation were delivered via git repository with a clearly viewable commit history of the entire development process.
7.	Offerors formally certified that "the team who developed and designed the coding submission are proposed as key personnel to perform work under the resulting task order barring any unforeseen circumstances"

### Result

Upon submission VA evaluated the development and design of each Offeror’s code submission.  The development component of the evaluation considered seven development categories, each with subcomponents.  The design component of the evaluation considered four categories, each with subcomponents.

These categories are listed below:

__Development__

1.	Security
2.	Testing
3.	Database/Data Modeling
4.	Code Quality
5.	Application Quality
6.	Documentation
7.	DevOps

__Design__

1.	Visual Design
2.	User Research
3.	Interaction Design
4.	Thoroughness

The team received 6 total submissions that each took 4-6 hours to evaluate. __The rubric and evaluation criteria allowed the technical team to select the vendor offering the best value and with the best expertise for the project requirements__:

1.	Allowed test of the submitted code for errors that would be fatal to project success
2.	Allowed determination of best code based on hard evidence.  For example, the team loaded 10,000 transactions into a user's account and measured the time it took to load their account page. Applications that took too long to load failed. 

### Next Steps

Iterate, Deliver, Iterate, Deliver, Iterate, Deliver…

### Lessons Learned/Additional Information

1.	Using a contracting vehicle with pre-vetted vendors narrows the pool to those with the best chance of success
2.	Within the evaluation the Government should be cautious about imposing go/no go wording.  For example, stating within Code Quality a requirement that “there are no flagrant misspellings or typos” can become very binary. 
3.	Consider whether your evaluation criteria allows for sufficient strengths and weaknesses.
4.	Discuss with your team whether not completing all user stories is a reason for a deficiency, or whether the quality of code and design could “offset” a missing user story.  Specifically, would the Government desire all user stories addressed with lesser quality and design, or almost all user stories completed with high quality and design.
5.	Much of the coding evaluation made for a very objective analysis (e.g. Specific security vulnerability identified), which is helpful to both parties for debriefing purposes.
6.	This approach requires expertise to evaluate successfully.  Without a development and design team this approach would not be effective.
7.	Depending on the acquisition, consider further limiting or eliminating the written approach portion.

### For more information contact the U.S. Digital Service Acquisitions team at <techfarhub@omb.eop.gov>
